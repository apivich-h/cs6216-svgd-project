\documentclass[final]{beamer}

% ====================
% Packages
% ====================

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[size=a0,scale=1.0]{beamerposter}
\usetheme{gemini}
\usecolortheme{ITNU}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{svg}
\usepackage{pgfplots}
\usepackage{caption}
\usepackage{subcaption}

% for circling parts
\usepackage{graphicx}
\usepackage[usestackEOL]{stackengine}
\usepackage{xcolor}
\def\calloutsym{%
  \ensurestackMath{%
  \scalebox{3.5}{\color{red}\stackunder[0pt]{\bigcirc}{\downarrow}}}%
}
\newcommand\callouttext[1]{%
  \def\stacktype{S}\renewcommand\useanchorwidth{T}\stackText%
  \stackunder{\calloutsym}{\scriptsize\Longstack{#1}}\stackMath%
}
\newcommand\callout[3][1.5pt]{%
  \def\stacktype{L}\stackMath\stackunder[#1]{#2}{\callouttext{#3}}%
}

% ====================
% Lengths
% ====================

% If you have N columns, choose \sepwidth and \colwidth such that
% (N+1)*\sepwidth + N*\colwidth = \paperwidth
\newlength{\sepwidth}
\newlength{\colwidth}
\setlength{\sepwidth}{0.001\paperwidth}
\setlength{\colwidth}{0.3\paperwidth}

\newcommand{\separatorcolumn}{\begin{column}{\sepwidth}\end{column}}

% ====================
% Title
% ====================

\title{CS6216 Project Report: Stein Variational Gradient Descent}

\author{Apivich Hemachandra\inst{1} \and
Jiashu Tao\inst{1} \and
Bo Wang\inst{1}   \and
Jiayuan Ye\inst{1} }
\institute{Department of Computer Science,  National University of Singapore\\{\small\textsuperscript{*}Alphabetical Order.}}

% remove this section if poster is for inhouse project
\addtobeamertemplate{headline}{} 
{
    \begin{tikzpicture}[remember picture,overlay]
    % tweak these sizes according to the logo of the company:
    % xshift, yshift, height
    \node [anchor=north west, inner sep=3cm] at ([xshift=-1.5cm,yshift=-0.8cm]current page.north west)     {\includegraphics[height=3cm]{imgs/logo.png}}; 
    \end{tikzpicture} 
}

% ====================
% Body
% ====================

\begin{document}

\begin{frame}[t]
\begin{columns}[t]

\separatorcolumn

\begin{column}{\colwidth}

\begin{block}{Problem Setting}

\begin{itemize}
    \item The aim is to generate a set of particles that provides a good approximation of some probability distribution. This is useful in Bayesian inference problem.
    
    \item The work uses Stein discrepancy to develop a variational inference method that works for a general inference problem and is also scalable.
\end{itemize}

\input{figtabstex/img-setting}

\end{block}

\begin{block}{SVGD Algorithm}

\textbf{Bayesian Inference via Variational Gradient Descent~\cite{liu2016stein}}

\begin{itemize}
    \item {\bfseries Input:} A target distribution with density function $p(x)$ and a set of initial particles $\{x_i^0\}_{i=1}^n$.
    \item {\bfseries Output:} A set of particles $\{x_i\}_{i=1}^n$ that approximates the target distribution.
    \item {\bfseries Iterative Particle update:} for iteration $\ell$, $x_i^{\ell+1}\leftarrow x_i^l + \epsilon_l \hat{\mathbf{\phi}}^*(x_i^\ell)$, 
	where $\hat{\mathbf{\phi}}^*(x) = \frac{1}{n} \sum_{j=1}^n\left[ k(x_{j}^\ell, x)\nabla_{x_j^\ell}\log p(x_j^\ell) + \nabla_{x_j^\ell} k(x_j^\ell, x)\right]$
\end{itemize}
\end{block}

  \begin{block}{Experiment: Toy Example of One-dimensional Gaussian Mixture}
    \input{figtabstex/fig-toy1d}
    
    \begin{enumerate}
        \item When the two modes are far apart from each other, the SVGD algorithm converges more slowly, as in Figure 1. (2).
        \item When the smaller mode is far away from the initialization distribution, the particles in the SVGD algorithm have difficulty visiting the smaller mode, as in Figure 1. (3).
        \item Replacing the weighted negative log-likelihood $\rightarrow$ a single loss function such as the ELBO: This ELBO-within-Stein algorithm is implemented NumPyro, and we found that when applying it to Gaussian mixture, the particles seem to converge faster and only need 100 iterations, as in Figure 1. (4). 
    \end{enumerate}
  \end{block}
  


\end{column}

\separatorcolumn

\begin{column}{\colwidth}

% \textbf{Convergence of SVGD after finite number of iteration}
    


\begin{block}{Experiment: SVGD vs Monte Carlo for Mean Estimation}
    \begin{enumerate}
    \item SVGD performs better than Monte Carlo sampling (Smaller MSE).
    \item A larger step size for SVGD might converge faster but might also lead to a larger error.
    \end{enumerate}
    \input{figtabstex/fig-toy1dmc}
\end{block}

  \begin{block}{Experiment: Running Time vs \#Particles}

    \begin{itemize}
     \item \textbf{Tested Implementations:} SVGD (Original paper), NumPyro SteinVI (ELBO Loss)
     \item \textbf{Task:} Matching a multivariate Gaussian \quad  \textbf{Number of Particles: 100\sim 1600}
    \end{itemize}
     \input{figtabstex/fig-timingparticles}
  \end{block}

\begin{block}{Experiment: Bayesian Logistic Regression on Small Datasets}
  \textbf{Logistic Regression:} On small datasets ($N \leq 10^4$)
    \begin{itemize}
     \item Compare the SVGD algorithm against the No U-Turn Sampler (NUTS) 
     
     \item Comparison using the accuracy of prediction and log-likelihood of test data
    \end{itemize}
     \input{figtabstex/fig-logisticsvgdnuts}
  \end{block}

\end{column}

\separatorcolumn

\begin{column}{\colwidth}

  \begin{block}{Experiment: Bayesian Logistic Regression on Large Datasets}
  \textbf{Bayesian Linear Regression} on binary Covertype dataset 
    \begin{itemize}
     \item SVGD \cite{liu2016stein} consistently outperforms SGLD \cite{welling2011bayesian} in test accuracy
     \item SVGD is also more particle efficient than SGLD
     \item Training is noisier than what the paper reported originally
    \end{itemize}
    \input{figtabstex/fig-covertype}
  \end{block}
  
  \begin{block}{Experiment: Bayesian Neural Network}
  \begin{itemize}
      \item Compare the RMSE and log-likelihood between SVGD implementations and the probabilistic back-propagation (PBP) algorithm
      
      \item We are able to show that the original implementation of SVGD algorithm outperforms PBP, however, the NumPyro implementation often cannot perform as well
  \end{itemize}
  \input{figtabstex/tab-bnn}
  \end{block}
  
% \begin{block}{Summary}
% \end{block}

  \begin{block}{References}

    \nocite{*}
    \footnotesize{\bibliographystyle{plain}\bibliography{poster}}

  \end{block}
  
\end{column}

\separatorcolumn

\end{columns}
\end{frame}

\end{document}