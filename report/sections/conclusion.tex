\section{Conclusion}
In this report, we summarize the technical details of SVGD. We also compare the performance of the SVGD algorithm with other inference algorithms on various tasks. We also test a more practical implementation of SVGD as available on a popular inference framework NumPyro.

If we have more time, we would consider more baselines and datasets for Bayesian logistic regression. One baseline is Doubly Stochastic Variational Inference (DSVI) proposed by \cite{ref_dsvi}. The original implementation is for MATLAB, and current attempt to reproduce it in Python is at debugging stage. Experiments with DSVI shows constant test accuracy despite of the gradient descent, suggesting the implementation has some problems.

With more time and resources, we also could have tested the SVGD algorithm performance on larger scale BNNs against other algorithms which are designed to perform inference on BNNs, such as the Monte Carlo dropout techniques. SVGD algorithm may not perform as well as some of these algorithm due to it being designed for a more general inference problem, however it would still be interesting to see if the accuracy and scalability is comparable to BNN-specific algorithms or not.