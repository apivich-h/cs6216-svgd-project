\section{Introduction}
Gradient descent has become one of the most popular optimization algorithms in many scenarios, particularly in deep learning. However, neural networks are known for not doing well as probabilistic models in Bayesian settings. The logits they output have no real means in terms of probability, but are rather "confidence scores". Having said that, gradient descent can be tweaked to work by redesigning the gradient function. The paper we choose to reproduce is titled "Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm" \cite{ref_article_svgd}. In this paper, Liu et al. propose a general-purpose variational inference algorithm that performs gradient descent to minimize the KL divergence between a set of particles and a target distribution. In this report, we first summarize the technical details of Stein variational gradient descent (SVGD) and how it can be integrated into Bayesian inferences. Then we repeat experiments done in the paper to verify the experimental results. In the end, we critically analyze our experimental results and summarize our findings. We also provide a reflection on what we could have done if we have more time. Our implementation is publicly accessible at \url{https://github.com/apivich-h/cs6216-svgd-project}.