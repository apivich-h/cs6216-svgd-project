\section{Experiments}
In this section, we report our reproduction effort and verification of the experimental results in the paper. For the SVGD algorithm, we modify and adapt the original implementation for our experiments. We also use the implementation of the algorithm on NumPyro for comparison purposes. The implementation on NumPyro is described further in Appendix \ref{ssect:svgd-npy}.

\subsection{Toy Example}
\input{figtabstex/fig-toy1d}

\noindent\textbf{Illustration of the Convergence of SVGD.} We first reproduced the toy example about the 1D Gaussian mixture mentioned in the paper. The task is to use SVGD to approximate a target distribution with PDF $p(x) = \frac{1}{3} N(x; -2, 1) + \frac{2}{3} N(x; 2, 1)$. The particles are initialized by an i.i.d. sample from the distribution $N(-10, 1)$, which is far away from the mode of the target distribution. We use 100 particles and 500 iterations with step size 0.25. We show in figure \ref{fig:toy1dgaussian}. (1), that SVGD effectively recovered the mixture distribution in $500$ epochs.

We perform additional experiments for two other Gaussian mixture distribution: $\frac{1}{3} N(x; -3, 1) + \frac{2}{3} N(x; 3, 1)$ (where the two modes are farther apart) and $\frac{2}{3} N(x; -3, 1) + \frac{2}{3} N(x; 3, 1)$ (where the smaller mode is farther from the initialization distribution). We see that when the two modes are far apart from each other, the SVGD algorithm converges more slowly, as in Figure  \ref{fig:toy1dgaussian}. (2). Moreover, when the smaller mode is far away from the initialization distribution, the particles in the SVGD algorithm have difficulty visiting the smaller mode, within 500 iterations, as in Figure  \ref{fig:toy1dgaussian}. (3). We also replace the weighted negative log-likelihood (over $n$ particles) in the SVGD Algorithm, with a single loss function that captures the $f$-divergence between the discrete particles and the target distribution p(x), such as ELBO. This ELBO-within-Stein algorithm is implemented NumPyro, and we found that when applying it to Gaussian mixture, the particles seem to converge faster and only need 100 iterations, as in Figure  \ref{fig:toy1dgaussian}. (4).

\noindent\textbf{Illustration of the Performance and Scalability of SVGD.} The paper also found that SVGD performs better than Monte Carlo sampling on some expectation estimation tasks (smaller MSE). We implement this experiment based on our implementation of the toy example. We use both SVGD and MC to get samples/particles in the same size and check averaged MSE of three estimators estimating $E[X]$, $E[X^2]$, and $E[cos(\omega x + b)]$ respectively. We follow the same setting as in the original paper, except that we calculate averaged MSE over 400 runs for each setting. The results confirmed the claim in the paper, as shown in figure \ref{fig:toy1dmc}.

\input{figtabstex/fig-toy1dmc}

We observe that while SVGD performs better than MC, it is significantly slower. To know how scalable it is with a large number of particles, we did an additional experiment (which does not exist in the original paper) to plot the running time of using a different number of particles to match an unknown multivariate Gaussian in 1000 iterations. We implement two versions of the code, one using the original SVGD implementation in the paper, the other using NumPyro's SteinVI API with ELBO loss. We vary the number of particles from 100 to 1600. We run it on AMD Threadripper 3970X 32-Core Processor with 64GB of RAM and a GeForce RTX 3090 GPU. We found that the 
running time is quadratic with the number of particles, as shown in figure \ref{fig:timingparticles}.

\input{figtabstex/fig-timingparticles}

\subsection{Bayesian Logistic Regression}
\subsubsection{Small scale}
We attempt to test the SVGD algorithm in small-scale (less than $10^4$ data points) logistic regression tasks. We aim to construct a model which is able to predict a binary label of some training data. We run the SVGD algorithm as implemented on NumPyro against the No U-Turn Sampler (NUTS) algorithm \cite{nuts}. For the NUTS algorithm, we use 1000 particles for the burn-in period, and 1000 generated samples. For the SVGD algorithm, we run for 100 iterations on 100 particles, similar to the original experiment. For our experiments, we constructed a model representing the logistic regression problem using NumPyro primitives, and use NUTS and SVGD algorithm implementation on NumPyro to perform inference. The result is shown in Figure \ref{fig:logist_small}. We can see that the performance is about the same between both algorithms, but SVGD requires fewer particles to represent the distribution. We also note, however, that similar to previously, the SVGD algorithm on NumPyro requires more time to achieve the performance. We present the time data in Table \ref{tab:logist-time} in Appendix \ref{ssect:bylog-data}.

\input{figtabstex/fig-logisticsvgdnuts}

\subsubsection{Large scale}
We also attempt to compare the performance of SVGD with other optimization algorithms on large-scale Bayesian logistic regression in terms of test accuracy with respect to the number of iterations and particle size. We use the same dataset the original paper used, which is the binary Covertype dataset. The dataset contains 581012 data entries and each data point has 54 features. The binary Covertype dataset is a special variant of the original Covertype dataset which has 7 output classes. The binary version combines two output classes out of the 7 into class 1, and treats the rest classes as class 0. This dataset is very large even for current computers. If we do not use stochastic gradient descent and load all data in one batch, we would most likely encounter memory overflow. 

The baseline we choose is stochastic gradient Langevin dynamics (SGLD) \cite{ref_sgld}, where a noise term is added to the gradient in each step. In particular, we use the parallel version of SGLD. Fig \ref{fig:covertype} shows the evolution of test accuracy against training iterations and the number of particles. Fig \ref{fig:acc_iter} compares the test accuracy between SVGD and SGLD when both of them are training for 2 epochs (roughly 18,000 iterations). SVGD consistently outperforms SGLD, achieving higher accuracy after 1000 iterations than SGLD after 18,000 iterations. However, from our experiments, the training using SVGD is far noisier than SGLD, which was not observed in the original experiments. Fig \ref{fig:acc_par} compares the test accuracy between SVGD and SGLD when the particle size varies. For each particle size, we conduct 50 independent trials and report the average performance. Similar to Fig \ref{fig:acc_iter}, the curves are not as smooth as the original plot. However, the relative performance still holds: SVGD is more particle efficient than SGLD, which means we can afford to reduce the number of particles to save more time in training.
\input{figtabstex/fig-covertype}

\subsection{Bayesian Neural Network}

For the experiments, we train a small Bayesian neural network (BNN) on a set of regression tasks. We follow the setup from the paper and use the UCI dataset. The BNN trained has one hidden layer and has 50 units, except for the Protein dataset which instead uses a model with 100 units due to the higher data count. The training is done with a subsampling size of 100.

We run tests on the SVGD algorithm as implemented by the authors of the original paper and also the implementation on NumPyro, and compare the performances against the probabilistic back-propagation (PBP) algorithm \cite{pbp}. One change made for the algorithm is to use the same number of epochs across all algorithms. For the Kin8nm, Naval, Power, and Protein dataset, 50 epochs are used. The remaining dataset is trained on 200 epochs. The other hyperparameters were kept the same as the original code. Each experiment was repeated 10 times, except for the Protein dataset which was only repeated 5 times. We report the root-mean-squared error (RMSE) and the log-likelihood values of the test data in Tables \ref{tab:bnn}. 

\input{figtabstex/tab-bnn}

In our results, we were able to show that SVGD performs better than the PBP algorithm. In most test cases used, we are able to show that the predictions from the SVGD algorithm is more accurate than that from the PBP algorithm, as measured by the root-mean-squared error of the prediction. 

However, we found that the SVGD algorithm implementation on NumPyro was not able to replicate the performance that the original SVGD algorithm was able to. This could be due to the fact that unlike the SVGD algorithm from the original paper, the SVGD algorithm on NumPyro is minimizing the loss based on the ELBO value, and not the true posterior probability. We also find that the two implementations require different tunings of the hyperparameters to obtain the best performances. While the original SVGD implementation tends to not find the minimum with step sizes larger than $10^{-3}$, for the NumPyro version, we had to increase the step size to $10^{-2}$ to get the best performance. 

We also report the amount of time required to run the algorithms in Table \ref{tab:bnn_time} in Appendix \ref{ssect:bnn-time}. We find that the original SVGD algorithm requires the least amount of time to compute. We also see that the NumPyro implementation of SVGD runs slower than the original implementation of SVGD due to the sampling required in the ELBO estimation process. The amount of sampling required for the ELBO estimate could have been reduced, however at a cost of a worse estimate of the value. However, we do note that the ELBO estimation could have been also been run in parallel, which we have not done for our tests.

In addition to experiments from the original paper, we also try to train the BNN while varying the number of epochs the training runs for. We plot some of these sessions in Figure \ref{fig:bnn-epoch} in Appendix \ref{ssect:bnn-epoch}. We were able to see that in the experiments, the SVGD algorithm may sometimes converge slowly to the true posterior. This may be due to the lower learning rate, which has been set in order for the samples to converge, although at the cost of slower convergence. As a result, we see that the SVGD algorithm will likely outperform the PBP algorithm if trained for long enough.