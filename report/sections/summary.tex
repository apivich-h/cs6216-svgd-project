\section{Summary of the Paper}

\paragraph{Stein Variation Gradient Descent (SVGD) Algorithm.}

\begin{enumerate}
    \item Goal: find a distribution $q(x)$ that minimizes its KL divergence with regard to the target distribution $p(x)$ on $\mathcal{X}=\mathbb{R}^d$.
    \item Functional gradient descent: iteratively update $q(x)$ to be another push-forward distribution $q_{[\epsilon\mathbf{\phi}]}(x) = T_{\#}q(x)$, that is the distribution of $T(x)$ for $x\sim q(x)$, where $T= I + \epsilon \mathbf{\phi}$ is a mapping with stepsize $\epsilon$. Here $\phi(x): \mathbb{R}^d\rightarrow \mathbb{R}^d$ could be any smooth mapping.
    \item Steepest descent direction and Stein's discrepancy: find the mapping $\mathbf{\phi}(x)$ that results in the steepest descent of KL divergence $\arg\max_{\phi} \frac{d}{d \epsilon} KL(q_{[\epsilon\mathbf{\phi}]}\left(x)\lVert p(x)\right)$. The author translate this objective to be the following Stein's discrepancy,
    \begin{align}
    \label{eqn:objective}
         \arg\max_\phi\frac{d}{d \epsilon}KL(q_{[\epsilon\mathbf{\phi}]}\left(x)\lVert p(x)\right) = \arg\max_\phi \mathbb{E}_{x\sim q}\left[trace\left(\mathcal{A}_p \mathbf{\phi}(x)\right)\right]
    \end{align}
    where the operator $\mathcal{A}_p \phi(x)= \nabla \log p(x) \cdot \phi(x) + \nabla\phi(x)$.
    \item Kernel trick for closed form solution: Let $\mathcal{H}$ be a reproducing kernel Hilbert space with positive definite kernel $k(x,x')$, i.e. $\mathcal{H}$ is the closure of the linear span $\{f: f = \sum_{i=1}^m a_i \cdot k(x, x_i), a_i\in \mathbb{R}, m\in \mathbb{N}, x_i\in \mathcal{X}=\mathbb{R}^d \}$. Then for $\phi(x) = \left(\phi_1(x), \phi_2(x), \cdots, \phi_d(x)\right)^T \in \mathcal{H}^d$, s.t. $\lVert\phi\rVert_{\mathcal{H}_d}\leq S(q, p)$, \eqref{eqn:objective}  has the following closed form solution.
    \begin{align}
    \label{eqn:solution}
        \phi(x) \propto \mathbb{E}_{x'\sim q}\left[ \mathcal{A}_p k(x, x') \right]
    \end{align}
    where $S(q, p) = \max_{\phi\in \mathcal{H}^d, s.t. \lVert\phi\rVert_2\leq 1} \mathbb{E}_{x\sim q}\left[trace\left(\mathcal{A}_p \mathbf{\phi}(x)\right)\right]$ is the Kernelized Stein's discrepancy. Here the authors assume that the function $K(x, x')$ given fixed $x'$ lies in the stein class of target density function $p(x)$.
    \item Approximate steepest descent under discrete particles: 
    
\end{enumerate}


\paragraph{Extreme case of SVGD with $n=1$ particle.} This is equivalent to MAP estimate under gradient descent of $\log p(x)$ where $p(x)$ is the target distribution.