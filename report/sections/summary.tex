\section{Summary of the Paper}

\paragraph{Stein Variation Gradient Descent (SVGD) Algorithm.}

\begin{enumerate}
    \item Goal: find a distribution $q(x)$ that minimizes its KL divergence with regard to the target distribution $p(x)$ on $\mathbb{R}^d$.
    \item Functional gradient descent: iteratively update $q(x)$ to be another push-forward distribution $q_{[\epsilon\mathbf{\phi}]}(x) = T_{\#}q(x)$, that is the distribution of $T(x)$ for $x\sim q(x)$, where $T= I + \epsilon \mathbf{\phi}$ is a mapping with stepsize $\epsilon$. Here $\phi(x): \mathbb{R}^d\rightarrow \mathbb{R}^d$ could be any smooth mapping.
    \item Steepest descent direction and Stein's method: find the mapping $\mathbf{\phi}(x)$ that results in the steepest descent of KL divergence $\arg\max_{\phi} \frac{d}{d \epsilon} KL(q_{[\epsilon\mathbf{\phi}]}\left(x)\lVert p(x)\right)$. The author translate the objective $\frac{d}{d \epsilon}KL(q_{[\epsilon\mathbf{\phi}]}\left(x)\lVert p(x)\right) = \mathbb{E}_p\left[trace\left(\mathcal{A}_p \mathbf{\phi}(x)\right)\right]$ to be the Stein's discrepancy, where the operator $\mathcal{A}_p \phi(x)= \nabla \log p(x) \cdot \phi(x) + \nabla\phi(x)$.
    \item Kernel trick for efficient approximation: Characterize $\phi(x)$ in reproducing kernel Hilbert space, s.t. $\phi(x) = \left(\phi_1(x), \phi_2(x), \cdots, \phi_d(x)\right)$, where $\phi_i(x) = a_{i1} K(x, x_1) + a_{i2} K(x, x_2) + \cdots + a_{in} K(x, x_n)$, with $n$ fixed particles $x_1,\cdots,x_n$ and a positive definite kernel $K(x,x')$ \footnote{The authors assume that the function $K(x, x')$ given fixed $x'$ lies in the stein class of proposal distribution's density function $p(x)$}.
    \item 
    
\end{enumerate}


\paragraph{Extreme case of SVGD with $n=1$ particle.} This is equivalent to MAP estimate under gradient descent of $\log p(x)$ where $p(x)$ is the target distribution.